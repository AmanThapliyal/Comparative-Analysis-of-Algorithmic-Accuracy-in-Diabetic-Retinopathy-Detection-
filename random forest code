import os
import cv2
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.decomposition import PCA

# Define paths and classes
data_path = r'Train'
classes = ['Mild', 'Moderate', 'NO DR', 'Proliferative', 'Severe']

# Function to load images and labels
def load_images_and_labels(data_dir, classes):
    images = []
    labels = []
    for label in classes:
        label_dir = os.path.join(data_dir, label)
        if os.path.isdir(label_dir):
            print(f"Processing directory: {label_dir}")  # Debugging line
            for image_name in os.listdir(label_dir):
                image_path = os.path.join(label_dir, image_name)
                image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
                if image is not None:
                    image = cv2.resize(image, (128, 128))  # Resize to 128x128 pixels
                    images.append(image.flatten())  # Flatten the image
                    labels.append(label)
                else:
                    print(f"Failed to load image: {image_path}")  # Debugging line
        else:
            print(f"Directory {label_dir} does not exist or is not a directory.")  # Debugging line
    return np.array(images), np.array(labels)

# Load data
X, y = load_images_and_labels(data_path, classes)

# Debugging information
print(f"Number of images loaded: {len(X)}")  # Check the number of images loaded
print(f"Shape of image data: {X.shape}")  # Check the shape of the data array
print(f"Labels: {set(y)}")  # Check the unique labels

# Check if images were loaded correctly
if X.size == 0:
    raise ValueError("No images were loaded. Please check the data path and image format.")

# Ensure that the data is in 2D format (n_samples, n_features)
if X.ndim != 2:
    raise ValueError(f"Expected a 2D array, but got a {X.ndim}D array instead.")

# Dimensionality reduction using PCA
pca = PCA(n_components=20)  # Reduce to 50 dimensions
X_pca = pca.fit_transform(X)
print(f"Shape after PCA: {X_pca.shape}")  # Check the shape after PCA

# Train the Random Forest model and perform Grid Search for hyperparameter tuning
rf_model = RandomForestClassifier()
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}
grid_search_rf = GridSearchCV(rf_model, param_grid, cv=5)
grid_search_rf.fit(X_pca, y)

# Best model after tuning
best_rf = grid_search_rf.best_estimator_
print("Best model parameters:", grid_search_rf.best_params_)  # Debugging line

# Test the model on the same data
y_pred_rf = best_rf.predict(X_pca)

# Calculate and print accuracy
accuracy_rf = accuracy_score(y, y_pred_rf)
print(f'Accuracy: {accuracy_rf:.2f}')

# Print classification report
print(classification_report(y, y_pred_rf, target_names=classes))

# Print confusion matrix
print(confusion_matrix(y, y_pred_rf))
